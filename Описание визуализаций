Были проведены два эксперимента:

1. Влияние параметра дисконтирования γ (0.95, 0.99, 0.999) на эффективность обучения алгоритма PPO.
   Гипотеза: при увеличении γ агент будет учитывать более долгосрочные награды, но обучение станет менее стабильным.
2. Сравнение алгоритмов PPO и A2C при одинаковом γ = 0.99.
   Гипотеза: PPO обучается стабильнее и достигает более высокой средней награды, чем A2C.

Среда: Pendulum-v1
Алгоритмы: PPO, A2C
Политика: MlpPolicy (2 слоя по 64 нейрона)
Количество шагов: 150 000
Seed: 42

Результаты экспериментов показали:
PPO с γ = 0.95 достиг наилучшей средней награды −177.18 ± 97.26, тогда как A2C с тем же γ = 0.99 показал худший результат −1561.77 ± 45.63.
Увеличение γ до 0.999 привело к снижению стабильности обучения и ухудшению результатов.
Алгоритм PPO подтвердил гипотезу о большей устойчивости и качестве обучения по сравнению с A2C.

Таким образом, наилучший результат достигнут при γ = 0.95, что указывает на оптимальный баланс между краткосрочным и долгосрочным планированием.
Видео работы лучшего агента сохранено в файле PPO_g095_episode.mp4.
