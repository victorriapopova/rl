# rl

# RL Project — Track 1 (Pendulum-v1)

## Описание задачи
Обучение агента в среде Pendulum-v1 с использованием алгоритмов PPO и A2C из библиотеки Stable-Baselines3.  
Цель — удерживать маятник в вертикальном положении, минимизируя отклонение и скорость вращения.

## Эксперименты
1. Влияние коэффициента дисконтирования γ (0.95, 0.99, 0.999) для PPO.  
   Гипотеза: при увеличении γ агент будет учитывать более долгосрочные вознаграждения, но обучение станет менее стабильным.  
2. Сравнение PPO и A2C при одинаковом γ=0.99.  
   Гипотеза: PPO обучается стабильнее и достигает лучших результатов.

## Параметры
- Среда: Pendulum-v1  
- Алгоритмы: PPO, A2C  
- Политика: MlpPolicy (2 слоя × 64 нейрона)  
- Количество шагов: 150 000  
- Seed: 42  

## Результаты
| Модель | Средняя награда | Стандартное отклонение |
|---------|-----------------|------------------------|
| PPO γ=0.95 | −177.18 ± 97.26 |
| PPO γ=0.99 | −998.53 ± 45.49 |
| PPO γ=0.999 | −1004.90 ± 177.47 |
| A2C γ=0.99 | −1561.77 ± 45.63 |

**Лучший агент:** PPO с γ=0.95

## Анализ
Меньшее значение γ (0.95) обеспечило более быстрое и устойчивое обучение.  
При γ=0.999 агент дольше ищет стратегию, а A2C показал худшую стабильность.  
PPO подтвердил ожидаемое преимущество в стабильности и качестве обучения.

## Видео финального агента
Файл: `results_track1/videos/PPO_g095_episode.mp4`

## Воспроизводимость
- Seed: 42  
- Установить зависимости:
  ```bash
  pip install -r requirements.txt
